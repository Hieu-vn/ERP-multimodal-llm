# -*- coding: utf-8 -*-
"""
Core RAG (Retrieval-Augmented Generation) pipeline.
This module orchestrates the retrieval, prompt formatting, and language model generation.
"""
import sys
from pathlib import Path
import asyncio
from tenacity import retry, stop_after_attempt, wait_fixed, wait_random_exponential, retry_if_exception_type

print("Script started.") # Added for debugging

# Add the project root to the Python path for robust imports
project_root = Path(__file__).resolve().parent.parent
sys.path.append(str(project_root))

from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain.agents import AgentExecutor, create_react_agent
from langchain import hub # ƒê·ªÉ t·∫£i prompt cho agent
from erp_ai_core.tools import get_current_date, graph_erp_lookup, VectorSearchTool, LiveERP_APITool, DataAnalysisTool # Import c√°c c√¥ng c·ª•
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline # For re-ranking
from erp_ai_core.agent_sales import get_product_stock_level, create_order, get_order_status, get_customer_outstanding_balance
from erp_ai_core.agent_inventory import get_inventory_overview, stock_in, stock_out, inventory_check, get_low_stock_alerts
from erp_ai_core.agent_finance import get_revenue_report, get_expense_report, get_customer_debt, create_receipt, create_payment
from erp_ai_core.agent_project_management import create_project, get_project_details, update_project_status, create_task, assign_task
from erp_ai_core.agent_workflow_automation import trigger_workflow, get_workflow_status, approve_workflow_step
from erp_ai_core.agent_hrm import create_employee, get_employee_profile, submit_leave_request, calculate_payroll, create_performance_goal
from erp_ai_core.agent_crm import create_lead, qualify_lead, create_opportunity, create_customer_account, create_support_ticket
from erp_ai_core.agent_computer_use import auto_create_purchase_order, auto_generate_report, auto_data_entry

from config.rag_config import RAGConfig

# Prompt for query rewriting/expansion
QUERY_REWRITE_PROMPT = PromptTemplate.from_template(
    """You are a helpful AI assistant. Your task is to rephrase or expand the user's question to improve retrieval from a knowledge base.
    Consider synonyms, related concepts, and different ways of asking the same question.
    Generate up to 3 alternative questions, one per line. If the original question is already good, just repeat it.

    Question: {question}
    Rewritten Questions:
    """
)

# Define a retry decorator for LLM calls
# This will retry up to 3 times with exponential backoff for any Exception
llm_retry = retry(
    stop=stop_after_attempt(3),
    wait=wait_random_exponential(multiplier=1, min=4, max=10),
    retry=retry_if_exception_type(Exception),
    reraise=True
)

class RAGPipeline:
    """A professional, end-to-end RAG pipeline for the ERP assistant."""

    def __init__(self):
        """Initializes the pipeline components to None. Call setup() to load them."""
        self.config = RAGConfig()
        self.vector_store = None
        self.llm = None
        self.chain = None
        print("RAG Pipeline initialized. Call setup() to load models and data.")

    def setup(self):
        """Loads all necessary components: vector store, embedding model, and LLM."""
        print("--- Setting up RAG Pipeline ---")

        # 1. Load the persistent vector store
        print(f"Attempting to load vector store from: {self.config.vector_store_path}")
        try:
            embedding_function = SentenceTransformerEmbeddings(model_name=self.config.embedding_model_name or "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
            self.vector_store = Chroma(
                persist_directory=self.config.vector_store_path,
                embedding_function=embedding_function,
                collection_name=self.config.collection_name
            )
            print("Vector store loaded successfully.")
        except Exception as e:
            print(f"Error loading vector store: {e}")
            raise # Re-raise to stop execution if vector store fails

        # 2. Initialize the LLM.
        # We will now attempt to load a fine-tuned model, falling back to a base model if not found.
        print(f"Attempting to initialize LLM (base: {self.config.base_model_name})...")
        
        try:
                from transformers import AutoModelForSeq2SeqLM, AutoModelForCausalLM, AutoTokenizer, pipeline
                from peft import PeftModel
                import torch
                import os

                model_name = self.config.base_model_name
                is_causal_lm = False # Assume seq2seq for T5, will be true for Llama/Gemma

                # Determine model type and load accordingly
                if "t5" in model_name.lower() or "flan" in model_name.lower():
                    ModelClass = AutoModelForSeq2SeqLM
                    task = "text2text-generation"
                else: # Assume causal LM for others like Llama, Gemma
                    ModelClass = AutoModelForCausalLM
                    task = "text-generation"
                    is_causal_lm = True

                print(f"Loading base model: {model_name} with {ModelClass.__name__}")
                load_dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float32
                load_in_4bit_enabled = False # Set to False for now

                model = ModelClass.from_pretrained(
                    model_name,
                    torch_dtype=load_dtype,
                    load_in_4bit=load_in_4bit_enabled
                )
                tokenizer = AutoTokenizer.from_pretrained(model_name)

                # Load adapter weights if a fine-tuned model path is provided and exists
                finetuned_model_path = self.config.finetuned_model_path
                if finetuned_model_path and os.path.exists(finetuned_model_path):
                    print(f"Loading fine-tuned adapter weights from: {finetuned_model_path}")
                    model = PeftModel.from_pretrained(model, finetuned_model_path)
                    print("Fine-tuned model loaded successfully.")
                else:
                    print(f"Fine-tuned model not found at '{finetuned_model_path}'. Using base model.")

                if is_causal_lm and tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token
                elif not is_causal_lm and tokenizer.pad_token is None:
                    tokenizer.pad_token = tokenizer.eos_token

                # Configure pipeline_kwargs based on task
                pipeline_kwargs = {
                    "max_new_tokens": 256,
                    "max_length": 512, # Keep for T5, will be less critical for larger context models
                    "temperature": 0.7, # Keep temperature for now, will be handled by pipeline
                    "top_k": 50,
                }
                if is_causal_lm: # Add do_sample and top_p for causal LMs
                    pipeline_kwargs["do_sample"] = True
                    pipeline_kwargs["top_p"] = 0.95

                # Create the transformers pipeline object
                hf_pipeline = pipeline(
                    task=task,
                    model=model,
                    tokenizer=tokenizer,
                    **pipeline_kwargs
                )

                self.llm = HuggingFacePipeline(pipeline=hf_pipeline)
                print("LLM initialized successfully.")
        except ImportError as e:
            print(f"Warning: Could not import necessary libraries for fine-tuned model loading ({e}). Falling back to basic HuggingFacePipeline.")
            self.llm = HuggingFacePipeline.from_model_id(
                model_id=self.config.llm_placeholder_model_name,
                task="text2text-generation",
                pipeline_kwargs={"max_new_tokens": 256, "top_k": 50, "temperature": 0.1},
            )
            print("LLM initialized with placeholder model.")
        except Exception as e:
            print(f"Error initializing LLM with fine-tuned model: {e}. Falling back to basic HuggingFacePipeline.")
            self.llm = HuggingFacePipeline.from_model_id(
                model_id=self.config.llm_placeholder_model_name,
                task="text2text-generation",
                pipeline_kwargs={"max_new_tokens": 256, "top_k": 50, "temperature": 0.1},
            )
            print("LLM initialized with placeholder model.")

        # 3. Initialize the Re-ranker
        print("Attempting to initialize Re-ranker...")
        try:
            reranker_model_name = self.config.reranker_model_name
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(reranker_model_name)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(reranker_model_name)
            self.reranker_pipeline = pipeline(
                "text-classification",
                model=self.reranker_model,
                tokenizer=self.reranker_tokenizer,
                device=0 if torch.cuda.is_available() else -1, # Use GPU if available
            )
            print("Re-ranker initialized successfully.")
        except Exception as e:
            print(f"Error initializing Re-ranker: {e}. Re-ranking will be skipped.")
            self.reranker_pipeline = None

        # 4. Build the RAG chain using LangChain Agents
        print("Attempting to build RAG Agent...")
        try:
            # Initialize VectorSearchTool
            self.vector_search_tool_instance = VectorSearchTool(self.vector_store, self.config.retrieval_k)

            # Initialize LiveERP_APITool
            self.live_erp_api_tool_instance = LiveERP_APITool()

            # Initialize DataAnalysisTool
            self.data_analysis_tool_instance = DataAnalysisTool()

            # Store all potential tools as attributes for dynamic access in query()
            self.all_potential_tools = {
                "get_current_date": get_current_date,
                "vector_search": self.vector_search_tool_instance.vector_search,
                "graph_erp_lookup": graph_erp_lookup,
                # SALES AGENT
                "get_product_stock_level": get_product_stock_level,
                "create_order": create_order,
                "get_order_status": get_order_status,
                "get_customer_outstanding_balance": get_customer_outstanding_balance,
                # INVENTORY AGENT
                "get_inventory_overview": get_inventory_overview,
                "stock_in": stock_in,
                "stock_out": stock_out,
                "inventory_check": inventory_check,
                "get_low_stock_alerts": get_low_stock_alerts,
                # FINANCE AGENT
                "get_revenue_report": get_revenue_report,
                "get_expense_report": get_expense_report,
                "get_customer_debt": get_customer_debt,
                "create_receipt": create_receipt,
                "create_payment": create_payment,
                # PROJECT MANAGEMENT AGENT
                "create_project": create_project,
                "get_project_details": get_project_details,
                "update_project_status": update_project_status,
                "create_task": create_task,
                "assign_task": assign_task,
                # WORKFLOW AUTOMATION AGENT
                "trigger_workflow": trigger_workflow,
                "get_workflow_status": get_workflow_status,
                "approve_workflow_step": approve_workflow_step,
                # HRM AGENT
                "create_employee": create_employee,
                "get_employee_profile": get_employee_profile,
                "submit_leave_request": submit_leave_request,
                "calculate_payroll": calculate_payroll,
                "create_performance_goal": create_performance_goal,
                # CRM AGENT
                "create_lead": create_lead,
                "qualify_lead": qualify_lead,
                "create_opportunity": create_opportunity,
                "create_customer_account": create_customer_account,
                "create_support_ticket": create_support_ticket,
                # COMPUTER USE AGENT
                "auto_create_purchase_order": auto_create_purchase_order,
                "auto_generate_report": auto_generate_report,
                "auto_data_entry": auto_data_entry,
                # DATA ANALYSIS
                "perform_calculation": self.data_analysis_tool_instance.perform_calculation
            }

            # Define the prompt for the ReAct Agent (will be used dynamically in query())
            self.agent_prompt_template = """B·∫°n l√† tr·ª£ l√Ω AI chuy√™n nghi·ªáp cho h·ªá th·ªëng ERP, h·ªó tr·ª£ ti·∫øng Vi·ªát. Nhi·ªám v·ª• c·ªßa b·∫°n l√† tr·∫£ l·ªùi ch√≠nh x√°c, s·ª≠ d·ª•ng th√¥ng minh c√°c c√¥ng c·ª• nghi·ªáp v·ª• b√™n d∆∞·ªõi:\n\nB·∫°n c√≥ c√°c c√¥ng c·ª• sau:\n{tools}\n\n**H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng tool:**\n\nüîç **T√¨m ki·∫øm & Truy v·∫•n:**\n- **`vector_search`**: T√¨m ki·∫øm ki·∫øn th·ª©c t·ªïng qu√°t, quy tr√¨nh, ƒë·ªãnh nghƒ©a, ch√≠nh s√°ch\n- **`graph_erp_lookup`**: Truy v·∫•n d·ªØ li·ªáu quan h·ªá, s·ªë li·ªáu, danh s√°ch ph·ª©c t·∫°p\n- **`get_current_date`**: L·∫•y ng√†y hi·ªán t·∫°i\n\nüì¶ **Qu·∫£n l√Ω Kho & B√°n h√†ng:**\n- **`get_product_stock_level`**: Ki·ªÉm tra t·ªìn kho s·∫£n ph·∫©m\n- **`create_order`**: T·∫°o ƒë∆°n h√†ng m·ªõi\n- **`get_order_status`**: Ki·ªÉm tra tr·∫°ng th√°i ƒë∆°n h√†ng\n- **`get_inventory_overview`**: T·ªïng quan t·ªìn kho\n- **`stock_in`**: Nh·∫≠p kho\n- **`stock_out`**: Xu·∫•t kho\n- **`inventory_check`**: Ki·ªÉm k√™ kho\n- **`get_low_stock_alerts`**: C·∫£nh b√°o t·ªìn kho th·∫•p\n\nüí∞ **T√†i ch√≠nh & K·∫ø to√°n:**\n- **`get_revenue_report`**: B√°o c√°o doanh thu\n- **`get_expense_report`**: B√°o c√°o chi ph√≠\n- **`get_customer_debt`**: Ki·ªÉm tra c√¥ng n·ª£ kh√°ch h√†ng\n- **`get_customer_outstanding_balance`**: S·ªë d∆∞ c√¥ng n·ª£ kh√°ch h√†ng\n- **`create_receipt`**: L·∫≠p phi·∫øu thu\n- **`create_payment`**: L·∫≠p phi·∫øu chi\n- **`calculate_payroll`**: T√≠nh l∆∞∆°ng nh√¢n vi√™n\n\nüë• **Qu·∫£n l√Ω Nh√¢n s·ª± (HRM):**\n- **`create_employee`**: T·∫°o h·ªì s∆° nh√¢n vi√™n m·ªõi\n- **`get_employee_profile`**: L·∫•y th√¥ng tin nh√¢n vi√™n\n- **`submit_leave_request`**: N·ªôp ƒë∆°n xin ngh·ªâ ph√©p\n- **`create_performance_goal`**: T·∫°o m·ª•c ti√™u hi·ªáu su·∫•t\n\nüéØ **Qu·∫£n l√Ω D·ª± √°n:**\n- **`create_project`**: T·∫°o d·ª± √°n m·ªõi\n- **`get_project_details`**: Chi ti·∫øt d·ª± √°n\n- **`update_project_status`**: C·∫≠p nh·∫≠t tr·∫°ng th√°i d·ª± √°n\n- **`create_task`**: T·∫°o c√¥ng vi·ªác\n- **`assign_task`**: G√°n c√¥ng vi·ªác cho nh√¢n vi√™n\n\nü§ù **Qu·∫£n l√Ω Kh√°ch h√†ng (CRM):**\n- **`create_lead`**: T·∫°o lead m·ªõi\n- **`qualify_lead`**: ƒê√°nh gi√° ch·∫•t l∆∞·ª£ng lead\n- **`create_opportunity`**: T·∫°o c∆° h·ªôi b√°n h√†ng\n- **`create_customer_account`**: T·∫°o t√†i kho·∫£n kh√°ch h√†ng\n- **`create_support_ticket`**: T·∫°o ticket h·ªó tr·ª£\n\n‚öôÔ∏è **T·ª± ƒë·ªông h√≥a & Quy tr√¨nh:**\n- **`trigger_workflow`**: K√≠ch ho·∫°t workflow t·ª± ƒë·ªông\n- **`get_workflow_status`**: Ki·ªÉm tra tr·∫°ng th√°i workflow\n- **`approve_workflow_step`**: Ph√™ duy·ªát b∆∞·ªõc workflow\n\nüñ•Ô∏è **T·ª± ƒë·ªông h√≥a UI:**\n- **`auto_create_purchase_order`**: T·ª± ƒë·ªông t·∫°o ƒë∆°n mua h√†ng qua UI\n- **`auto_generate_report`**: T·ª± ƒë·ªông t·∫°o b√°o c√°o qua UI\n- **`auto_data_entry`**: T·ª± ƒë·ªông nh·∫≠p d·ªØ li·ªáu qua UI\n\nüßÆ **Ph√¢n t√≠ch & T√≠nh to√°n:**\n- **`perform_calculation`**: Th·ª±c hi·ªán c√°c ph√©p t√≠nh ph·ª©c t·∫°p\n\n**Quy t·∫Øc ch·ªçn tool:**\n1. ƒê·ªçc k·ªπ c√¢u h·ªèi ƒë·ªÉ hi·ªÉu r√µ nghi·ªáp v·ª•\n2. Ch·ªçn tool ph√π h·ª£p nh·∫•t v·ªõi domain (Sales, Finance, HR, Project, CRM)\n3. ∆Øu ti√™n tool chuy√™n bi·ªát h∆°n tool t·ªïng qu√°t\n4. C√≥ th·ªÉ s·ª≠ d·ª•ng nhi·ªÅu tool trong m·ªôt c√¢u tr·∫£ l·ªùi\n\nS·ª≠ d·ª•ng format reasoning nh∆∞ sau:\n\nQuestion: c√¢u h·ªèi ƒë·∫ßu v√†o\nThought: ph√¢n t√≠ch nghi·ªáp v·ª•, x√°c ƒë·ªãnh domain, ch·ªçn tool ph√π h·ª£p\nAction: t√™n tool\nAction Input: input cho tool (JSON format)\nObservation: k·∫øt qu·∫£ tr·∫£ v·ªÅ\n... (c√≥ th·ªÉ l·∫∑p l·∫°i nhi·ªÅu l·∫ßn n·∫øu c·∫ßn)\nThought: ƒê√£ c√≥ ƒë·ªß th√¥ng tin ƒë·ªÉ tr·∫£ l·ªùi\nFinal Answer: c√¢u tr·∫£ l·ªùi cu·ªëi c√πng v·ªõi tr√≠ch d·∫´n ngu·ªìn\n\nB·∫Øt ƒë·∫ßu!\n\nQuestion: {input}\nThought:{agent_scratchpad}\n"""
            # No self.chain initialization here, it will be done dynamically in query()
            print("RAG Agent components initialized. AgentExecutor will be built dynamically per query.")

    @llm_retry
    async def query(self, role: str, question: str) -> dict:
        """ 
        Queries the RAG Agent asynchronously.
        Returns a dictionary containing the answer and source documents.
        """
        # if not self.chain: # No longer needed as chain is built dynamically
        #     raise RuntimeError("Pipeline has not been set up. Please call setup() first.")

        thought_process = []
        extracted_source_docs = []
        answer = "" # Will be filled by agent

        try:
            # Step 1: Query Rewriting/Expansion
            print(f"Original Question: {question}")
            rewritten_questions = [question] # Start with original question
            if self.llm: # Only attempt rewriting if LLM is available
                try:
                    rewrite_chain = QUERY_REWRITE_PROMPT | self.llm | StrOutputParser()
                    rewritten_output = await rewrite_chain.ainvoke({"question": question})
                    # Split by newline and clean up, add to list
                    rewritten_questions.extend([q.strip() for q in rewritten_output.split('\n') if q.strip() and q.strip() != question])
                    rewritten_questions = list(set(rewritten_questions)) # Remove duplicates
                    print(f"Rewritten Questions: {rewritten_questions}")
                except Exception as e:
                    print(f"Error during query rewriting: {e}. Proceeding with original question.")
            
            # For now, we will just use the original question for the agent invocation
            # In a more advanced setup, the agent might iterate through rewritten questions
            # or the tools themselves might handle multiple queries.
            agent_invocation_question = question # Use original question for agent

            # --- RBAC Enforcement: Dynamically filter tools based on user role ---
            allowed_tool_names = self.config.ROLE_TOOL_MAPPING.get(role, self.config.ROLE_TOOL_MAPPING["default"])
            
            # Construct the list of actual tool functions/instances for the current role
            current_role_tools = []
            for tool_name in allowed_tool_names:
                tool_func = self.all_potential_tools.get(tool_name)
                if tool_func:
                    # Special handling for graph_erp_lookup to pass llm_model
                    if tool_name == "graph_erp_lookup":
                        current_role_tools.append(lambda q, r: tool_func(q, r, self.llm))
                    else:
                        current_role_tools.append(tool_func)
                else:
                    print(f"Warning: Configured tool '{tool_name}' not found in all_potential_tools.")

            if not current_role_tools:
                answer = f"Xin l·ªói, vai tr√≤ '{role}' c·ªßa b·∫°n kh√¥ng c√≥ quy·ªÅn truy c·∫≠p v√†o b·∫•t k·ª≥ c√¥ng c·ª• n√†o. Vui l√≤ng li√™n h·ªá qu·∫£n tr·ªã vi√™n."
                return {
                    "answer": answer,
                    "source_documents": [],
                    "thought_process": []
                }

            # Create the Agent with the filtered tools
            agent_prompt = PromptTemplate.from_template(self.agent_prompt_template)
            agent = create_react_agent(self.llm, current_role_tools, agent_prompt)
            
            # Create the AgentExecutor dynamically
            agent_executor = AgentExecutor(agent=agent, tools=current_role_tools, verbose=True, handle_parsing_errors=True, return_intermediate_steps=True)

            # Invoke the AgentExecutor
            agent_result = await agent_executor.ainvoke({"input": agent_invocation_question, "role": role}) # Pass role to agent if needed
            answer = agent_result.get("output", agent_result.get("answer", "Kh√¥ng th·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi t·ª´ Agent."))

            # Format the thought process for XAI and extract source documents from tool observations
            if "intermediate_steps" in agent_result:
                for step in agent_result["intermediate_steps"]:
                    action = step[0]
                    observation = step[1]
                    thought_process.append(f"Thought: {action.log.strip()}\nAction: {action.tool}\nAction Input: {action.tool_input}\nObservation: {observation}")

                    # Check if the vector_search tool was used and extract its output
                    if action.tool == "vector_search":
                        # Assuming observation contains the concatenated document content
                        # For now, we'll just add a placeholder indicating content came from vector search
                        extracted_source_docs.append({"page_content": observation, "metadata": {"source": "vector_search_tool_output"}})

        except Exception as e:
            print(f"Error invoking Agent: {e}")
            answer = f"Xin l·ªói, c√≥ l·ªói x·∫£y ra khi c·ªë g·∫Øng tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n: {e}"
        
        # Apply re-ranking if reranker_pipeline is available and there are extracted_source_docs
        if self.reranker_pipeline and extracted_source_docs:
            print("Applying re-ranking to retrieved documents...")
            # Prepare documents for re-ranking: list of (query, document_text) pairs
            # For simplicity, we'll use the original question as the query for re-ranking
            # In a more advanced setup, you might use a rewritten query or sub-queries.
            rerank_inputs = [[question, doc["page_content"]] for doc in extracted_source_docs]
            
            # Perform re-ranking
            # The output of the pipeline will be a list of dictionaries with 'label' and 'score'
            # We assume a binary classification model where 'LABEL_1' indicates relevance.
            rerank_results = self.reranker_pipeline(rerank_inputs)

            # Pair original documents with their re-ranking scores
            scored_documents = []
            for i, doc in enumerate(extracted_source_docs):
                # Find the score for LABEL_1 (relevant) or similar, depending on model output
                score = next((item["score"] for item in rerank_results[i] if item["label"] == "LABEL_1"), 0.0)
                scored_documents.append((score, doc))
            
            # Sort documents by score in descending order
            scored_documents.sort(key=lambda x: x[0], reverse=True)
            extracted_source_docs = [doc for score, doc in scored_documents]
            print(f"Documents re-ranked. Top score: {scored_documents[0][0] if scored_documents else 'N/A'}")

        return {
            "answer": answer,
            "source_documents": extracted_source_docs, # Updated to use extracted_source_docs
            "thought_process": thought_process
        }

# Example usage (for testing)
if __name__ == '__main__':
    async def main():
        pipeline = RAGPipeline()
        pipeline.setup()

        print("\n--- Performing test query ---")
        test_role = "warehouse_manager"
        test_question = "L√†m th·∫ø n√†o ƒë·ªÉ ki·ªÉm tra t·ªìn kho hi·ªán t·∫°i?"
                
        result = await pipeline.query(test_role, test_question)
                
        print(f"\nRole: {test_role}")
        print(f"Question: {test_question}")
        print(f"\nAnswer: {result['answer']}")
        print(f"\nSource Documents:")
        for doc in result['source_documents']:
            print(doc)


        print("\n--- Performing test query with no relevant docs ---")
        test_role_no_docs = "unauthorized_user"
        test_question_no_docs = "Ch√≠nh s√°ch l∆∞∆°ng th∆∞·ªüng l√† g√¨?"
        
        result_no_docs = await pipeline.query(test_role_no_docs, test_question_no_docs)
        
        print(f"\nRole: {test_role_no_docs}")
        print(f"Question: {test_question_no_docs}")
        print(f"\nAnswer: {result_no_docs['answer']}")
        print(f"\nSource Documents:")
        for doc in result_no_docs['source_documents']:
            print(doc)

    import asyncio
    asyncio.run(main())
